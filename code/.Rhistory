library(MuMIn)
library(stats)
library(hydroGOF)
library(ggplot2)
library(MuMIn)
library(stats)
library(hydroGOF)
library(ggplot2)
library(caret)
library(tdr)
library(glmulti)
#install.packages("performance")
library(performance)
library(hydroGOF)
# Step 0: Importing and cleaning data
# import data file here (in this case "all2018.csv")
original_df = read.csv(file.choose(), na.strings = c(-9999, 'NaN'))
# remove rows with N/As and -9999s
df = original_df[complete.cases(original_df),]
# change na. action; dredge will now fail if n/a
options(na.action = 'na.fail')
# clean up Field column so that fields are correctly labeled
levels(df$Field)[levels(df$Field) == 2] <- 'Muth2'
levels(df$Field)[levels(df$Field) == 3] <- 'Muth3'
levels(df$Field)[levels(df$Field) == 8] <- 'Muth8'
# Create log transformations of values
df$ln_ECeg18 = log(df$ECeg18)
df$ln_ND = log(df$ND)
df$ln_CD = log(df$CD)
df$ln_ED = log(df$ED)
df$ln_CRD = log(df$CRD)
df$ln_CCD = log(df$CCD)
glmulti("ln_ECeg18",c("ND","CD","ED","ln_ND","ln_CD","ln_ED"),df,level=2,method="d",crit="aicc",maxsize = 8)
GLresults <- glmulti("ln_ECeg18",c("ND","CD","ED","ln_ND","ln_CD","ln_ED"),df,level=2,method="g",crit="aicc",maxsize=8)
final_model <- GLresults@objects[[1]]
second_model <- GLresults@objects[[2]]
print(final_model)
final_model$formula
print(final_model$formula)
summary(final_model)
View(df)
compare_performance(final_model,second_model)
# Check model assumptions
# Coefficients and significance
# Summary plots
par(mfrow=c(2,2))
plot(final_model)
# Step 2: Take best model and create output predictions
df$ECeg18_pred = predict(final_model, newdata=df)
# Step 2: Take best model and create output predictions
df$ln_ECeg18_pred = predict(final_model, newdata=df)
df$ECeg18_pred = exp(df$ln_ECeg18_pred)  #back-transform from log
# Step 3: Run GOF stats on predictions
GOF_stats = function(df, ECeg18=T) {
# GOF stats generated via functions from hydroGOF package:
# https://www.rforge.net/doc/packages/hydroGOF/d.html
# ifelse statement to select the prediction of ECeg or not
if(ECeg18==T) {
obs = df$ECeg18
pred = df$ECeg18_pred
} else {
obs = df$ECe
pred = df$ECe_pred
}
# Root Mean Squared Error (RMSE)
RMSE_model = rmse(pred, obs)
# Normalized RMSE (NRMSE)
NRMSE_model = nrmse(pred, obs)
# Pearson's Correlation Statistic
Cor_model = cor(pred, obs)
# Willmott's index of agreement (IOA)
IOA_model = d(pred, obs)
print("full dataset statistics:###########################################")
Summary = data.frame(
Statistic = c('RMSE', 'NRMSE', 'Corr', 'IOA'),
Value = c(RMSE_model, NRMSE_model, Cor_model, IOA_model)
)
print(Summary)
}
GOF_stats(df=df, ECeg18 = T)
# Step 2: Take best model and create output predictions
df$ln_ECeg18_pred = predict(final_model, newdata=df)
df$ECeg18_pred = exp(df$ln_ECeg18_pred)  #back-transform from log
obs = df$ECeg18
pred = df$ECeg18_pred
RMSE_model = rmse(pred, obs)
pred
obs
typeof(pred)
typeof(obs)
pred+1
# Normalized RMSE (NRMSE)
NRMSE_model = nrmse(pred, obs)
#install.packages("performance")
library(performance)
# Root Mean Squared Error (RMSE)
RMSE_model = rmse(pred, obs)
# Normalized RMSE (NRMSE)
NRMSE_model = nrmse(pred, obs)
# Pearson's Correlation Statistic
Cor_model = cor(pred, obs)
# Willmott's index of agreement (IOA)
IOA_model = d(pred, obs)
print("full dataset statistics:###########################################")
Summary = data.frame(
Statistic = c('RMSE', 'NRMSE', 'Corr', 'IOA'),
Value = c(RMSE_model, NRMSE_model, Cor_model, IOA_model)
)
print(Summary)
# Root Mean Squared Error (RMSE)
RMSE_model = rmse(as.numeric(pred), obs)
# Root Mean Squared Error (RMSE)
RMSE_model = rmse(as.numeric(pred), as.numeric(obs))
detach("package:performance", unload=TRUE)
library(hydroGOF)
# Root Mean Squared Error (RMSE)
RMSE_model = rmse(pred, obs)
RMSE_model
# Step 3: Run GOF stats on predictions
GOF_stats = function(df, ECeg18=T) {
# GOF stats generated via functions from hydroGOF package:
# https://www.rforge.net/doc/packages/hydroGOF/d.html
# ifelse statement to select the prediction of ECeg or not
if(ECeg18==T) {
obs = df$ECeg18
pred = df$ECeg18_pred
} else {
obs = df$ECe
pred = df$ECe_pred
}
# Root Mean Squared Error (RMSE)
RMSE_model = rmse(pred, obs)
# Normalized RMSE (NRMSE)
NRMSE_model = nrmse(pred, obs)
# Pearson's Correlation Statistic
Cor_model = cor(pred, obs)
# Willmott's index of agreement (IOA)
IOA_model = d(pred, obs)
print("full dataset statistics:###########################################")
Summary = data.frame(
Statistic = c('RMSE', 'NRMSE', 'Corr', 'IOA'),
Value = c(RMSE_model, NRMSE_model, Cor_model, IOA_model)
)
print(Summary)
}
GOF_stats(df=df, ECeg18 = T)
#Calculate the Mean Bias Error
#MBE = tdStats(df$ECe18, df$ECe18_pred, functions = c("mbe"))
MBE = tdStats(df$ECeg18, df$ECeg18_pred, functions = c("mbe"))
MBE
# Step 4: Cross-validate output predictions using LOFO RMSPE
lofo = function(df, fxn){
rmspe_list = c()
field_list = c()
for(i in unique(df$Field)){
test = subset(df, Field != i)
lofo = subset(df, Field == i)
# extract formula and apply to new data
mdl = lm(eval(fxn$call[[2]]), data = test)
#y_pred = predict(fxn, newdata = lofo)
y_pred = exp(predict(fxn, newdata = lofo)) #back-transform from log
rmspe = sqrt(mean((lofo$ECeg18 - y_pred)^2))
rmspe_list = append(rmspe_list, rmspe)
field_list = append(field_list, i)
}
final_df = data.frame(Left_out_Field = field_list,
ECeg18_RMSPE_dSm = rmspe_list)
print(final_df)
}
lofo(df = df, fxn = final_model)
fxn = final_model
fxn
test = df
mdl = lm(eval(fxn$call[[2]]), data = test)
fxn$call[[2]]
as.forumla(fxn)
as.formula(fxn)
?eval
y_pred = exp(predict(fxn, newdata = df)) #back-transform from log
# Step 4: Cross-validate output predictions using LOFO RMSPE
lofo = function(df, fxn){
rmspe_list = c()
field_list = c()
for(i in unique(df$Field)){
test = subset(df, Field != i)
lofo = subset(df, Field == i)
# extract formula and apply to new data
mdl = fxn
#mdl = lm(eval(fxn$call[[2]]), data = test)
#y_pred = predict(fxn, newdata = lofo)
y_pred = exp(predict(mdl, newdata = lofo)) #back-transform from log
rmspe = sqrt(mean((lofo$ECeg18 - y_pred)^2))
rmspe_list = append(rmspe_list, rmspe)
field_list = append(field_list, i)
}
final_df = data.frame(Left_out_Field = field_list,
ECeg18_RMSPE_dSm = rmspe_list)
print(final_df)
}
lofo(df = df, fxn = final_model)
print(mean(final_df$ECeg18_RMSPE_dSm))
# Step 4: Cross-validate output predictions using LOFO RMSPE
lofo = function(df, fxn){
rmspe_list = c()
field_list = c()
for(i in unique(df$Field)){
test = subset(df, Field != i)
lofo = subset(df, Field == i)
# extract formula and apply to new data
mdl = fxn
#mdl = lm(eval(fxn$call[[2]]), data = test)
#y_pred = predict(fxn, newdata = lofo)
y_pred = exp(predict(mdl, newdata = lofo)) #back-transform from log
rmspe = sqrt(mean((lofo$ECeg18 - y_pred)^2))
rmspe_list = append(rmspe_list, rmspe)
field_list = append(field_list, i)
}
final_df = data.frame(Left_out_Field = field_list,
ECeg18_RMSPE_dSm = rmspe_list)
print(final_df)
print("Average RMSPE:")
print(mean(final_df$ECeg18_RMSPE_dSm))
}
lofo(df = df, fxn = final_model)
# Step 4: Cross-validate output predictions using LOFO RMSPE
lofo = function(df, fxn){
rmspe_list = c()
field_list = c()
for(i in unique(df$Field)){
test = subset(df, Field != i)
lofo = subset(df, Field == i)
# extract formula and apply to new data
mdl = fxn
#mdl = lm(eval(fxn$call[[2]]), data = test)
#y_pred = predict(fxn, newdata = lofo)
y_pred = exp(predict(mdl, newdata = lofo)) #back-transform from log
rmspe = sqrt(mean((lofo$ECeg18 - y_pred)^2))
rmspe_list = append(rmspe_list, rmspe)
field_list = append(field_list, i)
}
final_df = data.frame(Left_out_Field = field_list,
ECeg18_RMSPE_dSm = rmspe_list)
print(final_df)
print(Average RMSPE:)
print(mean(final_df$ECeg18_RMSPE_dSm))
}
lofo(df = df, fxn = final_model)
# Step 4: Cross-validate output predictions using LOFO RMSPE
lofo = function(df, fxn){
rmspe_list = c()
field_list = c()
for(i in unique(df$Field)){
test = subset(df, Field != i)
lofo = subset(df, Field == i)
# extract formula and apply to new data
mdl = fxn
#mdl = lm(eval(fxn$call[[2]]), data = test)
#y_pred = predict(fxn, newdata = lofo)
y_pred = exp(predict(mdl, newdata = lofo)) #back-transform from log
rmspe = sqrt(mean((lofo$ECeg18 - y_pred)^2))
rmspe_list = append(rmspe_list, rmspe)
field_list = append(field_list, i)
}
final_df = data.frame(Left_out_Field = field_list,
ECeg18_RMSPE_dSm = rmspe_list)
print(final_df)
print('Average RMSPE:')
print(mean(final_df$ECeg18_RMSPE_dSm))
}
lofo(df = df, fxn = final_model)
# Step 4 (alternate): Cross-validate output predictions via K-folds
set.seed(123)
train.control = trainControl(method = "repeatedcv", number = 10, repeats = 100)
model = train(final_model, data = df, method = "lm",
trControl = train.control)
# Train the model
model = train(eval(final_model$call[[2]]), data = df, method = "lm",
trControl = train.control)
model = train(eval(final_model), data = df, method = "lm",
trControl = train.control)
final_model$call[[2]]
model = train(final_model$call[[2]], data = df, method = "lm",
trControl = train.control)
model = train(final_model$formula, data = df, method = "lm",
trControl = train.control)
# Summarize the results
print(model)
exp(model$results[2])
# Step 6 (optional) 1:1 plot of results
par(pty="s")
qplot(ECeg18, ECeg18_pred, data = df, geom = 'point', color = Field) +
geom_abline(slope = 1) +
coord_fixed(xlim = c(0, max(df$ECeg18)), ylim = c(0, max(df$ECeg18))) +
#coord_fixed(xlim = c(0, max(df$ECeg18)), ylim = c(0, 40)) +
ggtitle("1:1 Plot of Observed and Model Predicted Vales \n2018 Dev, ECeg = f(NDVI, CRSI, ETD), log-log, No Interpo") +
xlab(expression(Observed~EC[eg]~dS~m^{-1})) +
ylab(expression(Predicted~EC[eg]~dS~m^{-1})) +
theme(plot.title = element_text(hjust = 0.5))
